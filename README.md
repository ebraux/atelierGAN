# Atelier GAN - Web scraping

![web scraping infographie](ressources/web-scraping.webp)

Le **web scraping** est une technique qui permet dâ€™extraire automatiquement des informations Ã  partir de pages web.
Il est utilisÃ©, par exemple, pour :

* rÃ©cupÃ©rer des prix ou des avis sur des sites e-commerce,
* collecter des articles ou donnÃ©es scientifiques,
* constituer des jeux de donnÃ©es pour lâ€™entraÃ®nement en machine learning (images, textes),
* surveiller lâ€™Ã©volution de contenus en ligne (mÃ©tÃ©o, sport, bourseâ€¦).

âš ï¸ Attention : le scraping doit toujours Ãªtre fait **dans le respect des conditions dâ€™utilisation du site et de la loi**.

---

## 1. AccÃ¨s Ã  la page web avec `requests`

La premiÃ¨re Ã©tape consiste Ã  tÃ©lÃ©charger le code HTML de la page. On utilise pour cela la bibliothÃ¨que **requests**.

### Code exemple

```python
import requests

url = 'https://generated.photos/faces'
response = requests.get(url)

if response.status_code == 200:
    print("SuccÃ¨s ! Contenu de la page rÃ©cupÃ©rÃ©.")
    print(response.text[:500])  # Affiche les 500 premiers caractÃ¨res
else:
    print(f"Erreur : statut {response.status_code}")
```

### Explication

* `requests.get(url)` â†’ envoie une requÃªte au serveur web.
* `response.status_code` â†’ vÃ©rifie que le code est `200` (succÃ¨s).
* `response.text` â†’ contient le code HTML de la page.

---

## 2. BeautifulSoup : installation et documentation

![logo BeautifulSoup](ressources/bs.png)

**BeautifulSoup** est une bibliothÃ¨que qui permet de lire et dâ€™extraire facilement des Ã©lÃ©ments HTML.

### Installation

```bash
pip install beautifulsoup4
```

ğŸ‘‰ Si besoin, installe aussi requests :

```bash
pip install requests
```

### Documentation officielle

[https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

---

## 3. Comprendre le scraping par balise HTML

Pour extraire des images, des titres, des liens, il faut :

1. **Examiner le HTML de la page**

   * Sur Chrome / Firefox â†’ clic droit â†’ *Inspecter* â†’ repÃ©rer les balises `<img>`, `<a>`, `<div>`, etc.
2. **Identifier les sÃ©lecteurs**

   * Exemple pour `https://generated.photos/faces` :

     ```html
     <a href="/face/...">
       <img src="https://images.generated.photos/..." alt="...">
     </a>
     ```

   â†’ On va cibler les `<img>` contenus dans les `<a>`.

---

## 4. Code exemple complet + explications

```python
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin
import time

# URL Ã  scraper
URL = 'https://generated.photos/faces'

# Dossier pour enregistrer les images
IMAGE_DIR = '../images'
os.makedirs(IMAGE_DIR, exist_ok=True)

# Headers pour simuler un vrai navigateur
HEADERS = {'User-Agent': 'Mozilla/5.0'}

def download_image(img_url, filename):
    response = requests.get(img_url, headers=HEADERS)
    if response.status_code == 200:
        with open(filename, 'wb') as f:
            f.write(response.content)
        print(f"Image enregistrÃ©e : {filename}")
    else:
        print(f"Erreur pour {img_url}")

def scrape_faces(url, max_images=5):
    response = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    img_tags = soup.select('a img')
    count = 0
    
    for img in img_tags:
        img_url = img.get('src')
        if img_url:
            full_url = urljoin(url, img_url)
            filename = os.path.join(IMAGE_DIR, f'face_{count + 1}.jpg')
            download_image(full_url, filename)
            count += 1
            if count >= max_images:
                break
            time.sleep(1)  # Pause pour respecter le serveur

scrape_faces(URL, max_images=5)
```

### Explications

* **`BeautifulSoup(response.text, 'html.parser')`** â†’ crÃ©e un objet analysable.
* **`soup.select('a img')`** â†’ sÃ©lectionne les balises `<img>` contenues dans des `<a>`.
* **`img.get('src')`** â†’ rÃ©cupÃ¨re lâ€™URL de lâ€™image.
* **`urljoin(url, img_url)`** â†’ reconstruit une URL complÃ¨te mÃªme si elle est relative.
* **`download_image()`** â†’ tÃ©lÃ©charge et sauvegarde lâ€™image.

### ExÃ©cuter le wep scraping
Le code doit Ãªtre lancÃ© Ã  partir du rÃ©pertoire /scrap
 ```bash
 cd scrap
 python scraper.py 
 ```

---

## 5. Bonnes pratiques du scraping

* Consulte toujours le fichier `robots.txt` du site pour vÃ©rifier ce qui est autorisÃ©.
* Respecte les conditions dâ€™utilisation du site.
* Ajoute un **User-Agent** pour simuler un vrai navigateur.
* Ne surcharge pas les serveurs â†’ utilise `time.sleep()` entre les requÃªtes.
* Limite le nombre de pages/images tÃ©lÃ©chargÃ©es.
* Garde une trace des URLs et des erreurs rencontrÃ©es.

---

## 6. Limites de BeautifulSoup

* BeautifulSoup ne voit que **le HTML initial**.
  Si le contenu est chargÃ© **par JavaScript** (comme souvent sur les sites modernes), il ne sera pas accessible.
* Pour ces cas, il faut utiliser :

  * **Selenium** â†’ simule un navigateur rÃ©el.
  * **Playwright** â†’ plus moderne et rapide.
* BeautifulSoup ne remplace pas un vrai moteur de rendu et ne gÃ¨re pas les interactions dynamiques.

---
